{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/nyp-sit/sdaai-iti107/blob/main/session-5/od_using_tfod_api/object_detection_using_tfod_api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" align=\"left\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V8-yl-s-WKMG"
   },
   "source": [
    "# Object Detection using Tensorflow Object Detection API (aka TFOD API)\n",
    "\n",
    "Welcome to the programming exercise of 'Object Detection using TFOD API'.  This notebook will walk you through, step by step, the process of using the TFOD API for object detection.\n",
    "\n",
    "Before you can run the codes in this notebook, ensure the TFOD API has been installed. If you are using the lab machine or the cloud VM that is provided, the TFOD API has been already been installed. If you are using your own machine, make sure to follow the [TFOD API installation instructions](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2.md) before you start. \n",
    "\n",
    "Ensure that you are using Tensorflow > 2.2 environment (activate tf2env if you are using cloud VM)\n",
    "\n",
    "*Credit: This notebook is adapted from the Object Detection Tutorial in the TFOD API.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kFSqkTCdWKMI"
   },
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "hV4P5gyTWKMI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following if you encountered the error message about cuDNN failed to initialize\n",
    "# You need to run the this immediately after importing tensorflow library\n",
    "from utils import fix_cudnn_bug\n",
    "\n",
    "fix_cudnn_bug()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wy72mWwAWKMK"
   },
   "source": [
    "## 2. Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r5FNuiRPWKMN"
   },
   "source": [
    "### TFOD API imports\n",
    "Here are the imports of the required object detection modules in TFOD API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "v7m_NY_aWKMK"
   },
   "outputs": [],
   "source": [
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cfn_tRFOWKMO"
   },
   "source": [
    "## 3. Model preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X_sEBLpVWKMQ"
   },
   "source": [
    "### Choose the detection model\n",
    "\n",
    "Any model exported using the `exporter_main_v2.py` tool of TFOD_API can be loaded here. We will cover the exporting tool in the next exercise when we do our own custom training.\n",
    "\n",
    "By default we use an \"SSD with Mobilenet\" model here. See the [detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md) for a list of other models that can be run out-of-the-box with varying speeds and accuracies. Note the filename of the downloaded file is in the format of \\<model name\\>.tar.gz, e.g. *faster_rcnn_resnet50_coco_2018_01_28.tar.gz*. Change the variable *MODEL_NAME* below to the \\<model name\\>, e.g. *faster_rcnn_resnet50_coco_2018_01_28*. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ai8pLZZWKMS"
   },
   "source": [
    "Now we download the pre-trained model from the model zoo and restore the model using keras api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KILYnwR5WKMS"
   },
   "outputs": [],
   "source": [
    "model_name = 'ssd_mobilenet_v2_320x320_coco17_tpu-8'\n",
    "model_url = \"http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz\"\n",
    "model_dir = tf.keras.utils.get_file(\n",
    "    fname=model_name, \n",
    "    origin=model_url,\n",
    "    untar=True,\n",
    "    cache_dir='.',\n",
    "    cache_subdir='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config = os.path.join(model_name, 'pipeline.config')\n",
    "model_dir = os.path.join(model_name, 'checkpoint')\n",
    "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
    "model_config = configs['model']\n",
    "detection_model = model_builder.build(\n",
    "      model_config=model_config, is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(\n",
    "      model=detection_model)\n",
    "ckpt.restore(os.path.join(model_dir, 'ckpt-0')).expect_partial()\n",
    "\n",
    "def get_model_detection_function(model):\n",
    "    \"\"\"Get a tf.function for detection.\"\"\"\n",
    "\n",
    "    @tf.function\n",
    "    def detect_fn(image):\n",
    "        \"\"\"Detect objects in image.\"\"\"\n",
    "\n",
    "        image, shapes = model.preprocess(image)\n",
    "        prediction_dict = model.predict(image, shapes)\n",
    "        detections = model.postprocess(prediction_dict, shapes)\n",
    "\n",
    "        return detections, prediction_dict, tf.reshape(shapes, [-1])\n",
    "\n",
    "    return detect_fn\n",
    "\n",
    "detect_fn = get_model_detection_function(detection_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need to provide the path to the appropriate label map file (explained later in 'Loading Label Map'). A list of label map files (with the file suffix .pbtxt) is provided in the `data` subfolder in the TFOD API object detection folder. So depending on the model you chose, copy the mapping file (.pbtxt) to appropriate working directory (e.g. the current directory of this notebook). In this lab, since we chose the model 'ssd_mobilenet_v2_320x320_coco17_tpu-8' which is trained on mscoco dataset, we will use the file 'mscoco_label_map.pbtxt'. This file has been copied to the current directory for your convenience. If you train your own custom detection model, you will need to provide your own label map file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the TFOD label_map_util to return dictionary mapping integers to appropriate string labels would be fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_LABELS = 'mscoco_label_map.pbtxt'\n",
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)\n",
    "\n",
    "# let's print out a few entries to see what are the different objects we have\n",
    "ids = [1, 50, 70]\n",
    "for id in ids: \n",
    "    print(category_index[id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_1MVVTcLWKMW"
   },
   "source": [
    "### Loading label map\n",
    "A 'Label map' maps indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility function of TFOD API, but anything that returns a dictionary mapping integers to appropriate string labels would be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "hDbpHkiWWKMX"
   },
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H0_1AGhrWKMc"
   },
   "source": [
    "## 4. Object Detection on Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed to display the images.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EFsoUHvbWKMZ"
   },
   "source": [
    "### Helper code\n",
    "\n",
    "The image is read using Pillow as an Image object. Image.size gives the dimension of image as widht, height ordering. `Image.getdata()` gives a flattened array of bytes, so we need to reshape it to `(height, width, channels)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the models that are trained with TFOD API, some standard tensor names are used, e.g. num_detections, detection_boxes, 'detection_scores', 'detection_classes', etc. \n",
    "\n",
    "The following codes assume the presence of the following tensors \n",
    "\n",
    "- detection_boxes: coordinates of the detection boxes in the image.\n",
    "- detection_scores: detection scores for the detection boxes in the image.\n",
    "- detection_classes: detection-level class labels.\n",
    "- num_detections: number of detections in the batch.\n",
    "\n",
    "In our case, our training specifies maximum total detections (max_total_detections) of 100 and also maximum detections per class (max_detections_per_class) of 100, the output tensors for detection_scores, detection_classes are of the shape (?,100) and for the detection_boxes it is (?, 100, 4) where the 4 refer to the diagonal corners of the bounding box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we read the image file using pillow Image class.  Remember that our network always expect the tensors to be fed in batches, we need to add additional dimension as first axis, by calling np.expand_dims(x, axis=0).\n",
    "\n",
    "We then call the detection function (`detect_fn`) obtained above to predict bounding boxes and classes.  We use the utility function provided by TFOD API: `visualization_utils.visualize_boxes_and_labels_on_image_array()` to draw the boxes on the image. We can control the score threshold for a box to be visualized by changing the `min_score_thresh` parameter value. \n",
    "\n",
    "If the label text is not clear or illegible, you may want to change the font used by the `visualize_boxes_and_labels_on_image_array()`. By default, it will try to load the font called arial.ttf and if there is an error in loading, it will then call `ImageFont.load_default()` and this default font may not be legible on certain platform (e.g. MacOS).  For more info on ImageFont, refers to [PIL documentation](https://pillow.readthedocs.io/en/stable/reference/ImageFont.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image_np):\n",
    "    input_tensor = tf.convert_to_tensor(\n",
    "        np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    detections, predictions_dict, shapes = detect_fn(input_tensor)\n",
    "    boxes = detections['detection_boxes'][0].numpy()\n",
    "    classes = detections['detection_classes'][0].numpy()\n",
    "    scores = detections['detection_scores'][0].numpy()\n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "          image_np,\n",
    "          boxes,\n",
    "          (classes + 1).astype(int),\n",
    "          scores,\n",
    "          category_index,\n",
    "          use_normalized_coordinates=True,\n",
    "          max_boxes_to_draw=200,\n",
    "          min_score_thresh=.50,\n",
    "          agnostic_mode=False)\n",
    "\n",
    "    display(Image.fromarray(image_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = 'data/dog.jpg'\n",
    "image_np = np.array(Image.open(image))\n",
    "predict(image_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Object Detection on Video (Optional) \n",
    "\n",
    "The following codes will perform detection real-time on video. It reads the video frame one by one and perform detection and draw the bounding boxes on each frame (image) and then display the image frame directly using cv2.imshow()\n",
    "\n",
    "Only run this when you are using a local computer, as the cv2 video player window is shown as a separate window on local computer, not within the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_for_video(video_filepath, detection_model):\n",
    "    video_player = cv2.VideoCapture(video_filepath)\n",
    "    while video_player.isOpened():\n",
    "        ret, image_np = video_player.read()\n",
    "        if ret:\n",
    "            input_tensor = tf.convert_to_tensor(\n",
    "            np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "            detections, predictions_dict, shapes = detect_fn(input_tensor)\n",
    "            viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "                  image_np,\n",
    "                  detections['detection_boxes'][0].numpy(),\n",
    "                  (detections['detection_classes'][0].numpy()+1).astype(int),\n",
    "                  detections['detection_scores'][0].numpy(),\n",
    "                  category_index,\n",
    "                  use_normalized_coordinates=True,\n",
    "                  max_boxes_to_draw=200,\n",
    "                  min_score_thresh=.50,\n",
    "                  agnostic_mode=False)\n",
    "\n",
    "            cv2.imshow('Object Detection', image_np)\n",
    "            if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Release camera and close windows\n",
    "    video_player.release()\n",
    "    cv2.destroyAllWindows() \n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_in_file = 'data/tube.mp4'\n",
    "run_inference_for_video(video_in_file, detection_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is slightly modified to read the video file frame by frame and perform detection on the frame and write the detected frame to a video file usig VideoWriter class provided by openCV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_video(video_in_filepath, video_out_filepath, detection_model):\n",
    "    if not os.path.exists(video_in_filepath):\n",
    "        print('video filepath not valid')\n",
    "    \n",
    "    video_reader = cv2.VideoCapture(video_in_filepath)\n",
    "    \n",
    "    nb_frames = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_h = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    frame_w = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "    video_writer = cv2.VideoWriter(video_out_filepath,\n",
    "                               cv2.VideoWriter_fourcc(*'XVID'), \n",
    "                               30.0, \n",
    "                               (frame_w, frame_h))\n",
    "\n",
    "    for i in tqdm(range(nb_frames)):\n",
    "        ret, image_np = video_reader.read()\n",
    "        input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "        detections, predictions_dict, shapes = detect_fn(input_tensor)\n",
    "        viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "                  image_np,\n",
    "                  detections['detection_boxes'][0].numpy(),\n",
    "                  (detections['detection_classes'][0].numpy()+1).astype(int),\n",
    "                  detections['detection_scores'][0].numpy(),\n",
    "                  category_index,\n",
    "                  use_normalized_coordinates=True,\n",
    "                  max_boxes_to_draw=200,\n",
    "                  min_score_thresh=.50,\n",
    "                  agnostic_mode=False)\n",
    "\n",
    "        video_writer.write(np.uint8(image_np))\n",
    "                \n",
    "    # Release camera and close windows\n",
    "    video_reader.release()\n",
    "    video_writer.release() \n",
    "    cv2.destroyAllWindows() \n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this code to create a video file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_in_file = 'data/london.mp4'\n",
    "video_out_file = 'data/london_detected.mp4'\n",
    "\n",
    "write_video(video_in_file, video_out_file, detection_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows how to perform detection using the webcam. You can only run this from your own laptop/computer, not from the cloud VM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Read frame from camera\n",
    "    ret, image_np = cap.read()\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    detections, predictions_dict, shapes = detect_fn(input_tensor)\n",
    "    #input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0))\n",
    "    #print(input_tensor.shape)\n",
    "    #detections  = detection_model(input_tensor)\n",
    "    \n",
    "    label_id_offset = 1\n",
    "    #image_np_with_detections = image_np.copy()\n",
    "\n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "          image_np,\n",
    "          detections['detection_boxes'][0].numpy(),\n",
    "          (detections['detection_classes'][0].numpy() + label_id_offset).astype(int),\n",
    "          detections['detection_scores'][0].numpy(),\n",
    "          category_index,\n",
    "          use_normalized_coordinates=True,\n",
    "          max_boxes_to_draw=200,\n",
    "          min_score_thresh=.5,\n",
    "          agnostic_mode=False)\n",
    "\n",
    "    # Display output\n",
    "    cv2.imshow('object detection', cv2.resize(image_np, (800, 600)))\n",
    "\n",
    "    # Check if 'q' key is hit, if yes, stop the video capture\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "object_detection_tutorial.ipynb?workspaceId=ronnyvotel:python_inference::citc",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "tf2env",
   "language": "python",
   "name": "tf2env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
