{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning the convolutional layers\n",
    "\n",
    "The code below shows how we can unfreeze last few layers to allow fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "from utils import prepare_data\n",
    "\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, roc_curve, \\\n",
    "                            precision_recall_curve, average_precision_score, confusion_matrix\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data\"\n",
    "models_path = \"models\"\n",
    "valid_size = 0.2\n",
    "FORCED_DATA_REWRITE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path, valid_path = prepare_data(data_path=data_path, \n",
    "                                      valid_size=valid_size, \n",
    "                                      FORCED_DATA_REWRITE=FORCED_DATA_REWRITE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg_path = os.path.join(train_path, \"Negative\")\n",
    "train_pos_path = os.path.join(train_path, \"Positive\")\n",
    "valid_neg_path = os.path.join(valid_path, \"Negative\")\n",
    "valid_pos_path = os.path.join(valid_path, \"Positive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height, img_width = 150, 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1. / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1305 images belonging to 2 classes.\n",
      "Found 325 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = datagen.flow_from_directory(train_path, \n",
    "                                        target_size=(img_height, img_width), \n",
    "                                        class_mode='binary', \n",
    "                                        batch_size=16, \n",
    "                                        shuffle=False)\n",
    "\n",
    "valid_gen = datagen.flow_from_directory(valid_path, \n",
    "                                        target_size=(img_height, img_width), \n",
    "                                        class_mode='binary', \n",
    "                                        batch_size=16, \n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained Model as Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base = VGG19(include_top=False, \n",
    "                         weights=\"imagenet\",  \n",
    "                         input_shape=(img_height, img_width, 3))\n",
    "\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg19 (Functional)           (None, 4, 4, 512)         20024384  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 20,287,553\n",
      "Trainable params: 20,287,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(units=512, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = False\n",
    "# only unfreeze the last conv block (block5_convx) \n",
    "for layer in conv_base.layers:\n",
    "    if layer.name in ['block5_conv4']:\n",
    "        layer.trainable = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg19 (Functional)           (None, 4, 4, 512)         20024384  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 20,287,553\n",
      "Trainable params: 263,169\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", \n",
    "                  optimizer=Adam(lr=0.0001), \n",
    "                  metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps_per_epoch = int(np.ceil(train_gen.n * 1. / train_gen.batch_size))\n",
    "valid_steps_per_epoch = int(np.ceil(valid_gen.n * 1. / valid_gen.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-19-0a884fc186f9>:7: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/20\n",
      "82/82 - 10s - loss: 0.7870 - accuracy: 0.4958 - val_loss: 0.6870 - val_accuracy: 0.5385\n",
      "Epoch 2/20\n",
      "82/82 - 8s - loss: 0.7706 - accuracy: 0.4904 - val_loss: 0.6800 - val_accuracy: 0.5662\n",
      "Epoch 3/20\n",
      "82/82 - 9s - loss: 0.7444 - accuracy: 0.5272 - val_loss: 0.6567 - val_accuracy: 0.6154\n",
      "Epoch 4/20\n",
      "82/82 - 9s - loss: 0.7170 - accuracy: 0.5632 - val_loss: 0.6642 - val_accuracy: 0.5908\n",
      "Epoch 5/20\n",
      "82/82 - 9s - loss: 0.7115 - accuracy: 0.5617 - val_loss: 0.6512 - val_accuracy: 0.6338\n",
      "Epoch 6/20\n",
      "82/82 - 9s - loss: 0.6919 - accuracy: 0.5724 - val_loss: 0.6386 - val_accuracy: 0.6431\n",
      "Epoch 7/20\n",
      "82/82 - 9s - loss: 0.6997 - accuracy: 0.5678 - val_loss: 0.6247 - val_accuracy: 0.6431\n",
      "Epoch 8/20\n",
      "82/82 - 9s - loss: 0.6829 - accuracy: 0.5931 - val_loss: 0.6516 - val_accuracy: 0.6185\n",
      "Epoch 9/20\n",
      "82/82 - 9s - loss: 0.6644 - accuracy: 0.5870 - val_loss: 0.6194 - val_accuracy: 0.6431\n",
      "Epoch 10/20\n",
      "82/82 - 9s - loss: 0.6593 - accuracy: 0.6107 - val_loss: 0.6136 - val_accuracy: 0.6462\n",
      "Epoch 11/20\n",
      "82/82 - 9s - loss: 0.6622 - accuracy: 0.6138 - val_loss: 0.6111 - val_accuracy: 0.6492\n",
      "Epoch 12/20\n",
      "82/82 - 9s - loss: 0.6538 - accuracy: 0.6130 - val_loss: 0.6102 - val_accuracy: 0.6646\n",
      "Epoch 13/20\n",
      "82/82 - 9s - loss: 0.6548 - accuracy: 0.6100 - val_loss: 0.6064 - val_accuracy: 0.6554\n",
      "Epoch 14/20\n",
      "82/82 - 9s - loss: 0.6341 - accuracy: 0.6429 - val_loss: 0.6065 - val_accuracy: 0.6677\n",
      "Epoch 15/20\n",
      "82/82 - 9s - loss: 0.6531 - accuracy: 0.6008 - val_loss: 0.6079 - val_accuracy: 0.6769\n",
      "Epoch 16/20\n",
      "82/82 - 9s - loss: 0.6594 - accuracy: 0.6000 - val_loss: 0.5996 - val_accuracy: 0.6708\n",
      "Epoch 17/20\n",
      "82/82 - 9s - loss: 0.6329 - accuracy: 0.6414 - val_loss: 0.5982 - val_accuracy: 0.6646\n",
      "Epoch 18/20\n",
      "82/82 - 9s - loss: 0.6139 - accuracy: 0.6567 - val_loss: 0.6107 - val_accuracy: 0.6923\n",
      "Epoch 19/20\n",
      "82/82 - 9s - loss: 0.6557 - accuracy: 0.6169 - val_loss: 0.6255 - val_accuracy: 0.6462\n",
      "Epoch 20/20\n",
      "82/82 - 9s - loss: 0.6142 - accuracy: 0.6483 - val_loss: 0.6003 - val_accuracy: 0.6800\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "      train_gen,\n",
    "      steps_per_epoch=train_steps_per_epoch,\n",
    "      epochs=20,\n",
    "      validation_data=valid_gen, \n",
    "      validation_steps=valid_steps_per_epoch,\n",
    "      verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation accuracy seems to be worse than before (i.e. without fine-tuning the convolutional layer and just trained the classification dense layer). One reason maybe that our training samples are too little. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "tf2env",
   "language": "python",
   "name": "tf2env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
